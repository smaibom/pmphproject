\section{Transformations}

\subsection{Array Expansion}
\label{sec:arrayexp}
To move the outer loop into the timeline loop on listing \ref{timeloop} and make the outer loop parallel, a number of array expansions in the global structure and local arrays in the rollback function is required.\\
Arrays which needs to be expanded with an outer dimension:
\begin{itemize}
\item In the loop in \verb!setPayoff! (Listing \ref{payoffloop}), the temporary variable \verb!payoff! is used to calculate
 \verb!myResult!. Since \verb!payoff! is declared in the outer nest, all the inner iterations depend on the same version of it. Hence
 it must be either privatised, array expanded, or calculations inlined.
\item The temporary arrays \verb!u! and \verb!v! in rollback, read values from \verb!myResult!, so they depend on the outer value.
\item Setting \verb!y! requires reads from \verb!u! and \verb!v!, so it depends on outer.
\item \verb!yy! is used for storing intermediate results in \verb!tridag! from arrays which may depend on the outer value.
\item The \verb!a!, \verb!b! and \verb!c! arrays are written to, and read from in each iteration of the outer loop.
\end{itemize}

The arrays \verb!myX!, \verb!myY!, \verb!myDxx!, \verb!myDyy! and \verb!myTimeline! do not need to be array expanded, there is no dependency on the value of the outer loop. myVarX and myVarY only depends on the timeline loop, which is sequential.

\subsection{Nest interchange and loop distribution}

\subsubsection{Structural}
\paragraph{Interchanging the outer loop inwards}
\subparagraph{Motivation:} In the original structure of the program, we have that a whole slew of loops
 (all of the loops in section \ref{sec:third}) are nested
 within the sequential time iterating loop (Listing \ref{timeloop}) which itself is positioned within
 the outer loop (Listing \ref{outerloop}). Meaning that we in general had 4-tiered loop nests of the following
 structure.
$$\mathrm{parallel} \to \mathrm{sequential} \to \mathrm{parallel} \to \mathrm{parallel}$$
We would prefer to have any sequential loops in the outer nests so that we can more effectively
 parallelise.
\subparagraph{Action:} Performed a nest interchange on the outer and time loops, moving the outer loop inwards.
\subparagraph{Result:} New structure where parallel loops are nested within the sequential loop.
$$\mathrm{sequential} \to \mathrm{parallel} \to \mathrm{parallel} \to \mathrm{parallel}$$
\subparagraph{Validity:} Moving a parallel loop inwards is always valid.

\paragraph{Distribution of Implicit x \& y loops}
\subparagraph{Motivation:} Currently the implicit x/y (Listing \ref{impxloop} \& \ref{impyloop}) consist of one or two loops along with a call
 to \verb!tridag!. We would prefer perfect loop nests.
\subparagraph{Action:} Distribute the second-most inner nest across the innermost nests.
\subparagraph{Result:} Two/three (for x/y) perfect loop nests.
\subparagraph{Validity:} All used arrays are already array expanded as per section \ref{sec:arrayexp}.
\paragraph{Distributing the outer loop}
\subparagraph{Motivation:} Moving the outer loop inwards, leaves us with a big parallel loop nesting smaller double nested parallel loop
 (Figure \ref{fig:bintouter}.
 These inner loops all have different dimensions making it difficult to make a cohesive parallelisation across them all. We would much
 prefer multiple triple-tiered loops with perfect loop nests.\\
 \begin{figure}[h!]
   \centering
\begin{tikzpicture}[every node/.style={draw,rectangle}]
     \node (time) {time}; \node [right= of time] (outer) {outer};
     \node [right= of outer, draw = none] (dot) {$\vdots$}; \node
     [above= 0.1cm of dot] (1) {loop 1}; \node [below= 0.1cm of dot]
     (2) {loop n};

     \path [->] (time) edge (outer) [->] (outer) edge (1) edge (2);
   \end{tikzpicture}
   \caption{Loop nesting before interchange of outer}
   \label{fig:bintouter}
 \end{figure}
\subparagraph{Action:} Distribute the outer loop across its contained loops.
\subparagraph{Result:} All loops that had an outer-dimension are now perfect loop nests with the structure in Figure \ref{fig:aintouter}.
\begin{figure}[h!]
  \centering
\begin{tikzpicture}[every node/.style={draw,rectangle}]
    \node (time) {time}; \node [right= of time, draw = none] (dot)
    {$\vdots$}; \node [above right= 0.1cm of dot] (1) {loop 1}; \node
    [below right= 0.1cm of dot] (2) {loop n}; \node [left= 0.2cm of 2]
    (outer2) {outer}; \node [left= 0.2cm of 1] (outer) {outer};

    \path [->] (time) edge (outer) edge (outer2) [->] (outer) edge (1)
    [->] (outer2) edge (2);
  \end{tikzpicture}
  \caption{Loop nesting after interchange of outer.}
  \label{fig:aintouter}
\end{figure}
\subparagraph{Validity:} This transformation is dependent on some of the already performed array expansions in section \ref{sec:arrayexp}.
 With these expansions, the transformation is valid. It does not change the order of execution within each iteration of the outer loop.
\subsubsection{Memory-motivated}
\label{memory}
\paragraph{Loop interchanging for memory coalescence}
\subparagraph{Motivation:} Given that our working set of arrays are all stored in row-major order, we have that subsequent columns in the
 same row are stored subsequently in memory. To take advantage of spatial-locality and CUDA's memory banks we wish to iterate over columns
 in our innermost nests. This is not the case in loops such as the explicit y loop (Listing \ref{eyloop}).
\subparagraph{Action:} Interchange the two innermost loop nests where needed.
\subparagraph{Result:} Mostly improved memory access patterns. However, some variables - such as \verb!myVarX! in (Listing \ref{exloop}) -
 are now accessed uncoalesced.
\subparagraph{Validity:} Parallel loops can always be interchanged inwards.
\paragraph{Merging explicit \& implicit x/y}
\subparagraph{Motivation:} One of the loops resulting from each of the distribution of the implicit x/y loops are now on the form.
\begin{lstlisting}
  for i = 1..n
   for j = 1..m
    a[i][j] = ...
    b[i][j] = ...
    c[i][j] = ...
   endfor
  endfor
\end{lstlisting}
 Each statement of each iteration of these loops reference the same memory location as the earlier explicit x/y
 (Listings \ref{exloop} and \ref{eyloop}) such as to \verb!myVarX! - and now have the same nest structure. We wish make these loads just
 once for each iteration.
\subparagraph{Action:} Merge the explicit x and y loops with the corresponding implicit loop nests. Expand \verb!a!, \verb!b!, and \verb!c!
 to two arrays, i.e. \verb!ax! and \verb!ay!, as they can no longer be reused.
\subparagraph{Result:} Larger x and y loops.
\subparagraph{Validity:} The \verb!a!, \verb!b!, and \verb!c! arrays had been reused, so the validity of the transformation is dependent on
 the expansion of the two arrays.
\subsection{Transposition}
In section \ref{memory} we interchanged loops for memory coalescence resulting in non coalesced access in \verb!myVarX!. To achieve coalesced access we transpose myVarX and read the transposed memory location.

\subsection{Tridag}
Tridag is not inherently a parallel function. However, it can be expressed as a sequence of scans interleaved with maps. We have
 been supplied with such a CUDA kernel, which we have applied to our parallelised program.

\subsection{CUDA}
After performing the transformations mentioned in this section all parallel loops in the program are now three-tiered perfect loop nests.
 Writing CUDA kernels for these loops now consists of the following.
 \begin{itemize}
 \item Flatten all arrays.
 \item If not already done for the needed arrays, allocate memory on the device and copy the arrays into it.
 \item Write a CUDA kernel consisting of the body of the loop in question.
 \item Launch the kernel with two dimensional blocks and three dimensional grids.
 \item Copy memory back to host if needed.
 \end{itemize}
Since nearly all computations are performed on device in our final version, we only have to copy the initialised arrays once into device
after initialisation and copy the \verb!myResult! array back in the end.